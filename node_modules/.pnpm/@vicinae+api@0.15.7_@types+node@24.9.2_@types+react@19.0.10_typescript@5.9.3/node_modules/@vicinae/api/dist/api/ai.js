"use strict";
Object.defineProperty(exports, "__esModule", { value: true });
exports.AI = void 0;
var AI;
(function (AI) {
    /**
     * Returns a prompt completion.
     *
     * @param prompt - The prompt to ask the AI.
     * @param options - Options to control which and how the AI model should behave.
     *
     * @example
     * ```typescript
     * import { Detail, AI, LaunchProps } from "@raycast/api";
     * import { usePromise } from "@raycast/utils";
     * import { useState } from "react";
     *
     * export default function Command(props: LaunchProps<{ arguments: { prompt: string } }>) {
     *   const [data, setData] = useState("");
     *   const { isLoading } = usePromise(
     *     async (prompt) => {
     *       const stream = AI.ask(prompt);
     *       stream.on("data", (data) => {
     *         setData((x) => x + data);
     *       });
     *       await stream;
     *     },
     *     [props.arguments.prompt]
     *   );
     *
     *   return <Detail isLoading={isLoading} markdown={data} />;
     * }
     * ```
     */
    function ask(prompt, options) {
        throw new Error('not implemented');
    }
    AI.ask = ask;
    /**
     * The AI model to use to answer to the prompt.
     * @defaultValue `AI.Model["OpenAI_GPT4o-mini"]`
     */
    let Model;
    (function (Model) {
        Model["OpenAI_GPT4"] = "openai-gpt-4";
        Model["OpenAI_GPT4-turbo"] = "openai-gpt-4-turbo";
        Model["OpenAI_GPT4o"] = "openai-gpt-4o";
        Model["OpenAI_GPT4o-mini"] = "openai-gpt-4o-mini";
        Model["Anthropic_Claude_Haiku"] = "anthropic-claude-haiku";
        Model["Anthropic_Claude_Opus"] = "anthropic-claude-opus";
        Model["Anthropic_Claude_Sonnet"] = "anthropic-claude-sonnet";
        Model["MixtraL_8x7B"] = "mixtral-8x7b";
        Model["Mistral_Nemo"] = "mistral-nemo";
        Model["Mistral_Large2"] = "mistral-large-2";
        Model["Llama3_70B"] = "llama3-70b";
        Model["Llama3.1_70B"] = "llama3.1-70b";
        Model["Llama3.1_8B"] = "llama3.1-8b";
        Model["Llama3.1_405B"] = "llama3.1-405b";
        Model["Perplexity_Llama3.1_Sonar_Huge"] = "perplexity-llama-3.1-sonar-huge-128k-online";
        Model["Perplexity_Llama3.1_Sonar_Large"] = "perplexity-llama-3.1-sonar-large-128k-online";
        Model["Perplexity_Llama3.1_Sonar_Small"] = "perplexity-llama-3.1-sonar-small-128k-online";
        /** @deprecated Use `AI.Model["OpenAI_GPT4o-mini"]` instead */
        Model["OpenAI_GPT3.5-turbo-instruct"] = "openai-gpt-3.5-turbo-instruct";
        /** @deprecated Use `AI.Model.Llama3_70B` instead */
        Model["Llama2_70B"] = "llama2-70b";
        /** @deprecated Use `AI.Model.Perplexity_Llama3_Sonar_Large` instead */
        Model["Perplexity_Sonar_Medium_Online"] = "perplexity-sonar-medium-online";
        /** @deprecated Use `AI.Model.Perplexity_Llama3_Sonar_Small` instead */
        Model["Perplexity_Sonar_Small_Online"] = "perplexity-sonar-small-online";
        /** @deprecated Use `AI.Model.Llama3_70B` instead */
        Model["Codellama_70B_instruct"] = "codellama-70b-instruct";
        /** @deprecated Use `AI.Model["Perplexity_Llama3.1_Sonar_Large"]` instead */
        Model["Perplexity_Llama3_Sonar_Large"] = "perplexity-llama-3-sonar-large-online";
        /** @deprecated Use `AI.Model["Perplexity_Llama3.1_Sonar_Small"]` instead */
        Model["Perplexity_Llama3_Sonar_Small"] = "perplexity-llama-3-sonar-small-online";
        /** @deprecated Use `AI.Model["OpenAI_GPT4o-mini"]` instead */
        Model["OpenAI_GPT3.5-turbo"] = "openai-gpt-3.5-turbo";
    })(Model = AI.Model || (AI.Model = {}));
    AI.getModels = async () => {
        throw new Error('not implemented');
    };
})(AI || (exports.AI = AI = {}));
